Docker： 启动container：  sudo docker run -t -i IMAGEPORT
list变为DS scala> val ds = List("abcdef", "abcd", "cdef", "mnop").toDS
1. 读取文件作为DataSet
scala> val textFile = spark.read.textFile("README.md")

2. Filter 函数   是一种 transformer
匿名函数的一种写法，因为在scala中函数可以作argument：
  因为这里filter的参数要求是 (Int => Boolean)
    val x = t.filter(argument => function body)
  
    所以可以这么写
    val x = t.filter(e => e%2 == 0)  #t是DataSet
                                     #选取符合的数

3. map 函数 是一种 transformer
也是匿名函数写法
假设map函数要求的 parameter 是(String => Int)   go from List[A] => List[B]. This is called lifting
val s : List[String] = List("abc", "a","ab")
val i : List[Int] = s.map(x => x.length)
上面是最简单的写法，你也可以写成
s.map((x : String) => x.length)

或者

val sl : (String => Int) = {
  x => x.length
}
val i = s.map(sl)

4. reduce function (op function)  (A, A) => A

List( 1, 2, 3, 4 ).reduce( (x,y) => x + y )
TextFile
 "hello Tyth"
 "cool example, eh?"
 "goodbye"

TextFile.map(line => line.split(" ").size)
 2
 3
 1
TextFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)  

5.flatMap()
Map transforms an RDD of length N into another RDD of length N. 
It is similar to Map, but FlatMap allows returning 0, 1 or more elements from map function.

flatMap() 
we call flatMap to transform a Dataset of lines to a Dataset of words
t2 = t.flatMap(line => line.split(""))



6.groupByKey（）  
t.flatMap(line => line.split(" ")).groupByKey(l=>l。substring(0,3)).count().collect()

以所有words前计算所有word前三个字母为key，统计个数。
反正就是类似的作用~~


Caching:  重复运行的放进去跑

      t.cache()   #之后跑就快了？？
      
      
7.从hive里面读取数据，得到的dataframe。dataframe和rdd都可以用rdd的transformer和action

val conf = new SparkConf().setAppName("SPARKHIVE").setMaster("local[*]")
val sc = new SparkContext(conf)
val hiveContext = new HiveContext(sc)
var df = hiveContext.sql("select msisdn,concat_ws('|',collect_set(col1),collect_set(col2)) as values from tb_input2 
where msisdn is not NULL group by msisdn")
//这里collect_set是会收集全部行，想不重复可以选其他的。
//concat_ws 类似于group_concat   用于合并多行

8.Ice接受mappartitions的数据问题:

一次action就会执行所有transformation操作，所以小心不要执行多次action
如果数据少，val processedDf = df.repartition(1).mapPartitions(sendNlp)
加一个repartition(1)设置partition为1个，这样就不会有空partition也穿到服务端了
