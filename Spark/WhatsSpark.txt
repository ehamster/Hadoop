1.Spark Core  
  除了是内存管理等，还提供api for RDD
2.Spark SQL
  就是提供了使用SQL语言管理结构化数据
3.Spark Streaming
  处理实时数据
4.MLlib
  common machine learning function
5.Graphx
6.Cluster Managers
Spark 可以运行在多种分布式上

Spark Core
1.有一个 driver program，里面有一个SparkContext
Use SparkContext to create RDD.
Spark将你的function传送到workNode
使用Spark
  1.建立SparkContext
  val conf = new SparkConf()。set Master（"local"）.setAppName("My App") // local表示没有链接到cluster
  val sc = new SparkContext(conf)
spark RDD

两种操作  transformation action

transformation是 lazy 的 不会立刻执行， 直到action操作才会执行
为了不重复执行， 可以a.persist
流程是
1.创建rdd
2.transformation
3.persist
4.action

一般读取用sc.textFile("/data/2.md")
